knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(base.dir = "/Users/Peter/Documents/GitHub/pwyckoff.github.io/", base.url = "/")
knitr::opts_chunk$set(fig.path = "images/")
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(lubridate))
suppressMessages(library(ggthemes))
suppressMessages(library(tidyverse))
suppressMessages(library("quanteda"))
suppressMessages(library("quanteda.textstats"))
suppressMessages(library("quanteda.textplots"))
suppressMessages(library(httr))
suppressMessages(library("rvest"))
suppressMessages(library("xml2"))
suppressMessages(library(plotly))
## Starting here so database does not need to be re-compiled.
df_compiled <- read.csv("C:/Users/Peter/Documents/analysis/climate_nyt/data/df_compiled.csv")
df_compiled <- df_compiled%>%
select(-X, -keep)
#head(df_compiled)
#Frequency of climate change articles over time
df_clim_freq <- df_compiled %>%
select(pub_date, print_page, news_desk, word_count, month_count)
df_clim_freq$date <- as_date(df_clim_freq$pub_date)
df_clim_freq$month <- month(df_clim_freq$date)
df_clim_freq$year <- year(df_clim_freq$date)
suppressMessages(df_clim_freq_date <- df_clim_freq %>%
group_by(year, month) %>%
summarise(month_share=n()/month_count*100))
df_clim_freq_date <- distinct(df_clim_freq_date)
df_clim_freq_date$date<- my(paste(df_clim_freq_date$month, "-", df_clim_freq_date$year))
#Going forward, colors from https://www.learnui.design/tools/data-color-picker.html
ggplot(data=df_clim_freq_date, aes(x=date, y=month_share))+
geom_point(color="#bc5090")+
geom_smooth(color="#ef5675")+
labs(title="Climate change articles as a share of total NYT articles, monthly, 1995-2022",
subtitle="Each dot represents the share of total articles in a month",
caption="Source: NYT API",
x="Date",
y="Share of total articles (%)")+
theme_bw()
df_clim_freq
df_clim_freq_date
df_clim_freq_date %>%
filter(year==1995)%>%
summarize(average=mean(month_share))
df_clim_freq_date %>%
filter(year==1995)%>%
group_by(year)%>%
summarize(average=mean(month_share))
df_clim_freq_date %>%
filter(year==2022)%>%
group_by(year)%>%
summarize(average=mean(month_share))
df_clim_freq_monthtot <- df_clim_freq %>%
group_by(month) %>%
summarise(monthtot=n())
ggplot()+
geom_point(data=df_clim_freq_monthtot, aes(x=month, y=monthtot), color="#7a5195", size=2)+
labs(title="Total number of climate change articles published in each calendar month,\nsum over 1999-2022",
subtitle="Months are from 1 (January) to 12 (December) on the x axis",
caption="Source: NYT API",
x="Month",
y="Sum of articles on climate change, 1999-2022")+
scale_x_continuous(breaks=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), limits=c(1, 12))+
scale_y_continuous(breaks=c(250, 500, 750), limits=c(50, 800))+
theme_bw()+
theme(panel.grid.minor = element_blank())
#Frequency by desk
#Combining major categories, and then creating an "other" option for desks that had fewer than 100 articles
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Opinion", "OpEd", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Arts", "Culture", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Arts & Ideas/Cultural Desk", "Culture", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Movies, Performing Arts/Weekend Desk", "Culture", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Arts&Leisure", "Culture", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="The Arts/Cultural Desk", "Culture", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Cultural Desk", "Culture", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Arts and Leisure Desk", "Culture", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Science / Environment", "Science", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Science Desk;", "Science", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Science Desk", "Science", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Travel Desk", "Travel", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="BookReview", "Books", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Books / First Chapters", "Books", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Book Review Desk", "Books", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Books / Sunday Book Review", "Books", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Business / Your Money", "Business", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Business Travel", "Business", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Business Day", "Business", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Business/Financial Desk", "Business", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="SundayBusiness", "Business", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Money and Business/Financial Desk", "Business", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="", "None", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Editorial Desk", "Editorial", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Foreign Desk", "Foreign", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Metro", "Metropolitan", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Metropolitan Desk", "Metropolitan", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Metropolitan", "Metro.", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="National Desk", "National", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Opinion", "OpEd", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Opinion", "OpEd", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Opinion", "OpEd", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk=="Opinion", "OpEd", df_clim_freq$news_desk)
df_clim_freq$news_desk<- ifelse(df_clim_freq$news_desk %in% c("Books", "Business", "Climate", "Culture", "Editorial", "Foreign", "Letters", "Metro.", "National", "None", "OpEd", "Science"), df_clim_freq$news_desk, "Other")
df_clim_freq$news_desk<-as.factor(df_clim_freq$news_desk)
table(df_clim_freq$news_desk)
ggplot(data=df_clim_freq, aes(x=news_desk, y=date))+
geom_violin(fill="#2f4b7c")+
scale_color_continuous()+
labs(title="Climate change articles over time, by news desk",
caption="Source: NYT API",
x="News Desk",
y="Date")+
theme_bw()
ggplot(data=df_clim_freq, aes(x=news_desk, y=date))+
geom_jitter(width=0.3, color="#2f4b7c", size=.5)+
labs(title="Climate change articles over time, by news desk",
caption="Source: NYT API",
x="News Desk",
y="Date")+
theme_bw()
#What page are they on?
df_clim_freq$print_page<-as.numeric(df_clim_freq$print_page)
df_clim_freq$print_page_group <- ifelse(df_clim_freq$print_page==1, "Front page", ifelse(df_clim_freq$print_page>1 &df_clim_freq$print_page<=10, "Pages 2-10", ifelse(df_clim_freq$print_page>10 & df_clim_freq$print_page<=50, "Pages 11-50", ifelse(df_clim_freq$print_page>50, "Pages 51+", "Other"))))
df_clim_freq_2 <- df_clim_freq %>%
group_by(year, month) %>%
mutate(average_page=mean(print_page, na.rm=TRUE))%>%
select(month, year, average_page)%>%
distinct()
df_clim_freq_2$date <- my(paste0(df_clim_freq_2$month, "/", df_clim_freq_2$year))
ggplot(data=df_clim_freq_2, aes(x=date, y=average_page))+
geom_point(color="#d45087")+
geom_smooth()+
labs(title="Average page rank, among articles about climate change",
caption="Source: NYT API",
x="Date",
y="Average Print Page Number")+
theme_bw()
#Number of days on front page in a month
suppressMessages(df_clim_freq_frontpage <- df_clim_freq %>%
filter(print_page==1)%>%
group_by(year, month) %>%
summarise(front_page_by_month=n()))
df_clim_freq_frontpage$date<- my(paste(df_clim_freq_frontpage$month, "-", df_clim_freq_frontpage$year))
ggplot(data=df_clim_freq_frontpage, aes(x=date, y=front_page_by_month))+
geom_point(color="#a05195")+
geom_smooth(color="#d45087")+
labs(title="Number of front page climate change articles by month",
subtitle="Each point represents one month in one year",
caption="Source: NYT API",
x="Date",
y="Number of front page climate change articles")+
theme_bw()
df_clim_freq_frontpage
df_clim_freq_frontpage%>%
group_by(year)%>%
summarize(average=mean(front_page_by_month))
ggplot(data=df_clim_freq_frontpage, aes(x=date, y=front_page_by_month))+
geom_point(color="#a05195")+
geom_smooth(color="#d45087")+
labs(title="Number of front page climate change articles by month",
subtitle="Each point represents one month in one year",
caption="Source: NYT API",
x="Date",
y="Number of front page climate change articles")+
theme_bw()
df_clim_freq_frontpage%>%
group_by(year)%>%
summarize(average=mean(front_page_by_month))
mean(2.22, 2.91, 2.81, 4.00, 3.00)
ggplot(data=df_clim_freq_frontpage, aes(x=date, y=front_page_by_month))+
geom_point(color="#a05195")+
geom_smooth(color="#d45087")+
labs(title="Number of front page climate change articles by month",
subtitle="Each point represents one month in one year",
caption="Source: NYT API",
x="Date",
y="Number of front page climate change articles")+
theme_bw()
df_clim_freq_frontpage%>%
group_by(year)%>%
summarize(average=mean(front_page_by_month))
df_clim_freq_frontpage%>%
group_by(year)%>%
summarize(average=mean(front_page_by_month))
df <- corpus(df_compiled$lead_paragraph, docvars=data.frame(headline=df_compiled$headlines, page=df_compiled$print_page, newsdesk=df_compiled$news_desk, section=df_compiled$section_name, wordcount=df_compiled$word_count, date=as_date(df_compiled$pub_date)))
#Worldcloud of all words
dfm1 <- df
dfm1 <- dfm1 %>%
tokens(remove_punct=TRUE, remove_numbers=TRUE) %>%
tokens_remove(stopwords("en")) %>%
tokens_wordstem()%>%
dfm()%>%
dfm_trim(min_termfreq = 2)
textplot_wordcloud(dfm1)
features_dfm1 <- textstat_frequency(dfm1, n = 50)
features_dfm1$feature <- with(features_dfm1, reorder(feature, -frequency))
ggplot(features_dfm1, aes(x = feature, y = frequency)) +
theme_bw()+
geom_point(color="#a05195") +
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
labs(title="Frequency of words in abstracts of climate change articles",
caption="Source: NYT API",
x="Word",
y="Number of times used in total set of articles")
df_texts_to_find <- df_disas_plot %>%
group_by(doc_id)%>%
mutate(all_disaster = sum(hurricane, storm, fire, drought, flood, cold, extreme, apocalyptic, disaster))
#Creating a dictionary for extreme weather
dict <- dictionary(list(hurricane=c("hurricane*", "typhoon*", "cyclone*"),
#rain=c("rain*", "precipitation", "squall*"),
storm=c("storm"),
fire=c("fire*", "wildfire*"),
drought=c("drought*"),
flood=c("flood*", "landslide*", "mudslide*"),
cold=c("icestorm", "snowstorm*", "blizzard*"),
extreme=c("extreme", "historic", "record"),
apocalyptic=c("apocalypse", "apocalyptic", "catastroph*"),
disaster=c("disaster*", "disastrous", "calamity", "devastation", "wreckage", "ravag*")
))
dfm_natural <- dfm_lookup(dfm1, dictionary=dict)
df_disas_plot <- convert(dfm_natural, to = "data.frame")
df_disas_plot$date <- dfm_natural$date
df_disas_plot$print_page <- dfm_natural$page
df_disas_plot$month<-month(df_disas_plot$date)
df_disas_plot$year <- year(df_disas_plot$date)
df_disas_plot_byyear <- df_disas_plot %>%
group_by(year) %>%
summarise(all_disaster = sum(hurricane, storm, fire, drought, flood, cold, extreme, apocalyptic, disaster))
ggplot(data=df_disas_plot_byyear, aes(x=year, y=all_disaster))+
geom_point(color="#ff7c43")+
labs(title="Number of climate change articles mentioning natural disasters, by year",
caption="Source: NYT API",
x="Year",
y="Number of articles")+
theme_bw()
df_texts_to_find <- df_disas_plot %>%
group_by(doc_id)%>%
mutate(all_disaster = sum(hurricane, storm, fire, drought, flood, cold, extreme, apocalyptic, disaster))
df$dictionary <- df_texts_to_find$all_disaster
#write.csv(convert(corpus_subset(df, dictionary>1), to = "data.frame"), "disaster_climate.csv")
#Go through urls and gather data on damages
disaster_wiki <- read.csv("C:/Users/Peter/Documents/analysis/climate_nyt/data/disaster_climate_wikidata.csv")
#Many abstracts either didn't mention a specific storm,  didn't have a url link, or were already referenced in a separate wikipedia article (eg. Hurricane Ida was covered in several articles). You can see the search terms used for each article in the appropriate column. I'll now filter down to the ones that have urls.
disaster_wiki <- disaster_wiki %>%
select(-X, -wikipedia_search_terms)%>%
filter(url!="")
#html_event <- read_html("https://en.wikipedia.org/wiki/1998_Florida_wildfires")
#title_elements_event <- html_event %>% html_elements(class = ".mw-page-title-main")
#html_text(title_elements_event)
#We are left with 29 articles
for (i in disaster_wiki$url){
html_event <- read_html(i)
label_elements_event <- html_event %>% html_elements(css = ".infobox-label")
data_elements_event <- html_event %>% html_elements(css = ".infobox-data")
label_event <- label_elements_event %>% html_text()
data_event <- data_elements_event %>% html_text()
event_table <- data.frame(label_event, data_event)
disaster_wiki$damage[disaster_wiki$url==i] <- ifelse (("Damage" %in% event_table$label_event == TRUE | "Property damage"%in% event_table$label_event == TRUE | "Cost" %in% event_table$label_event == TRUE | "Total damage" %in% event_table$label_event == TRUE), (event_table$data_event[event_table$label_event %in% c("Damage", "Property damage", "Cost", "Total damage")]), "")
disaster_wiki$lives[disaster_wiki$url==i] <- ifelse (("Deaths" %in% event_table$label_event == TRUE | "Fatalities" %in% event_table$label_event ==TRUE | "Total fatalities" %in% event_table$label_event == TRUE), (event_table$data_event[event_table$label_event %in% c("Deaths", "Fatalities", "Total fatalities")]), "")
title_elements_event <- html_event %>% html_elements(css = ".mw-page-title-main")
disaster_wiki$disaster[disaster_wiki$url==i] <- html_text(title_elements_event)
}
disaster_wiki$damage <- gsub("\\[.*\\]", "", disaster_wiki$damage)
disaster_wiki$casualties <- gsub("\\[.*\\]", "", disaster_wiki$lives)
disaster_wiki$casualties[15]<-""
disaster_wiki$casualties[19]<- "3,951+ deaths (Belgium: 716; France: 1,435; Germany: 500; Netherlands: 400; United Kingdom: 900)"
disaster_wiki$casualties[27] <- "≥1,408 deaths (estimated), ≥914 (confirmed). ~600-800 deaths in Canada (deadliest weather event in the history of Canada) and ~200-600 deaths in the US"
disaster_wiki$date<-mdy(disaster_wiki$date)
#change lives to casualties
#disaster_wiki$date<-as_date(disaster_wiki$date)
p <- ggplot(data=disaster_wiki, aes(x=date, y=wordcount, color=section, damages=damage, casualties=casualties, disaster=disaster))+
geom_point()+
labs(title="Climate change articles mentioning specific natural disasters, 1995-2022",
subtitle="Information about the natural disasters available in mouse-over",
caption="Source: NYT API, Wikipedia",
legend="NYT Section",
x="Date",
y="Article Word Count")+
theme_bw()
#ggplotly(p, tooltip=c("disaster", "damages", "casualties"))
p
ggplotly(p, tooltip=c("disaster", "damages", "casualties"))
dfm_natural
df_disas_plot
df_texts_to_find
suppressMessages(library(tidyverse))
#plot-related
suppressMessages(library(ggthemes))
suppressMessages(library(dplyr))
suppressMessages(library(ggrepel))
suppressMessages(library(plotly))
suppressMessages(library(scales))
suppressMessages(library(RColorBrewer))
#maps
suppressMessages(library(urbnmapr))
suppressMessages(library(devtools))
suppressMessages(devtools::install_github("UrbanInstitute/urbnmapr"))
suppressMessages(library(maps))
suppressMessages(library(mapdata))
suppressMessages(library(usmap))
suppressMessages(library(sf))
emissions_2006
emissions <- as.data.frame(read.csv("data/emissions_industry.csv"))
#head(emissions)
#Selecting main variables of interest here
emissions <- emissions %>%
select(year, naics_2012_code, emissions_total_intensity)
#head(emissions)
#Summary statistics on the data
#summary(emissions)
general_plot <- ggplot(emissions%>%
filter(year==2006))+
geom_point(aes(x=emissions_total_intensity, y=naics_2012_code))+
theme_bw()+
theme(panel.border = element_blank(), strip.background = element_blank())+
labs(title="Emissions intensity varies significantly by industry code",
x="Total emissions intensity, MmtCO2/b.$.",
y= "Industry code, 6-digit 2012 NAICS",
caption= "Note: MmtCO2/b.$. is millions of metric tons per billion dollars of real gross output using constant 2007 dollars \n Source: Department of Commmerce")
general_plot
emissions_2006 <- emissions %>%
filter(year==2006)
emissions_2006 %>%
sort()
emissions_2006
emissions_2006 %>%
sort(emissions_total_intensity)
emissions_2006 %>%
order(emissions_total_intensity)
emissions_2006 %>%
arrange(emissions_total_intensity)
emissions_2006 %>%
arrange(desc(emissions_total_intensity))
